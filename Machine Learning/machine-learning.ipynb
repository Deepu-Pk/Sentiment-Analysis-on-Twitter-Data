{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:51:52.579938Z",
     "iopub.status.busy": "2022-04-21T14:51:52.579440Z",
     "iopub.status.idle": "2022-04-21T14:51:52.603960Z",
     "shell.execute_reply": "2022-04-21T14:51:52.603305Z",
     "shell.execute_reply.started": "2022-04-21T14:51:52.579857Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the data in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:51:52.605664Z",
     "iopub.status.busy": "2022-04-21T14:51:52.605407Z",
     "iopub.status.idle": "2022-04-21T14:51:53.505609Z",
     "shell.execute_reply": "2022-04-21T14:51:53.504890Z",
     "shell.execute_reply.started": "2022-04-21T14:51:52.605630Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:51:53.507075Z",
     "iopub.status.busy": "2022-04-21T14:51:53.506749Z",
     "iopub.status.idle": "2022-04-21T14:51:53.619907Z",
     "shell.execute_reply": "2022-04-21T14:51:53.619243Z",
     "shell.execute_reply.started": "2022-04-21T14:51:53.507037Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Preprocessing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:51:53.622173Z",
     "iopub.status.busy": "2022-04-21T14:51:53.621851Z",
     "iopub.status.idle": "2022-04-21T14:51:54.077111Z",
     "shell.execute_reply": "2022-04-21T14:51:54.076437Z",
     "shell.execute_reply.started": "2022-04-21T14:51:53.622137Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:51:54.078853Z",
     "iopub.status.busy": "2022-04-21T14:51:54.078546Z",
     "iopub.status.idle": "2022-04-21T14:51:54.298820Z",
     "shell.execute_reply": "2022-04-21T14:51:54.298147Z",
     "shell.execute_reply.started": "2022-04-21T14:51:54.078819Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:51:54.300375Z",
     "iopub.status.busy": "2022-04-21T14:51:54.300104Z",
     "iopub.status.idle": "2022-04-21T14:51:54.304660Z",
     "shell.execute_reply": "2022-04-21T14:51:54.303804Z",
     "shell.execute_reply.started": "2022-04-21T14:51:54.300339Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import unicodedata as udata\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking the versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:51:54.306516Z",
     "iopub.status.busy": "2022-04-21T14:51:54.306221Z",
     "iopub.status.idle": "2022-04-21T14:51:54.315078Z",
     "shell.execute_reply": "2022-04-21T14:51:54.314147Z",
     "shell.execute_reply.started": "2022-04-21T14:51:54.306450Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sklearn.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(numpy.__version__)\n",
    "print(pd.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:51:54.317210Z",
     "iopub.status.busy": "2022-04-21T14:51:54.316347Z",
     "iopub.status.idle": "2022-04-21T14:51:59.904814Z",
     "shell.execute_reply": "2022-04-21T14:51:59.904052Z",
     "shell.execute_reply.started": "2022-04-21T14:51:54.317129Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/dataset-demo/TwitterSentimentAnalysis.csv\", encoding='latin-1', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the data in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:51:59.906529Z",
     "iopub.status.busy": "2022-04-21T14:51:59.906265Z",
     "iopub.status.idle": "2022-04-21T14:52:01.121091Z",
     "shell.execute_reply": "2022-04-21T14:52:01.120354Z",
     "shell.execute_reply.started": "2022-04-21T14:51:59.906478Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning the Columns name to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:01.124631Z",
     "iopub.status.busy": "2022-04-21T14:52:01.124354Z",
     "iopub.status.idle": "2022-04-21T14:52:01.128649Z",
     "shell.execute_reply": "2022-04-21T14:52:01.127667Z",
     "shell.execute_reply.started": "2022-04-21T14:52:01.124594Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:01.130554Z",
     "iopub.status.busy": "2022-04-21T14:52:01.130261Z",
     "iopub.status.idle": "2022-04-21T14:52:01.142249Z",
     "shell.execute_reply": "2022-04-21T14:52:01.141507Z",
     "shell.execute_reply.started": "2022-04-21T14:52:01.130519Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Null values in the dataset. Here we are counting each cloumn null values in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:01.143112Z",
     "iopub.status.busy": "2022-04-21T14:52:01.142917Z",
     "iopub.status.idle": "2022-04-21T14:52:02.005442Z",
     "shell.execute_reply": "2022-04-21T14:52:02.004744Z",
     "shell.execute_reply.started": "2022-04-21T14:52:01.143089Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the duplicates values and counting duplicates in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:02.007695Z",
     "iopub.status.busy": "2022-04-21T14:52:02.007261Z",
     "iopub.status.idle": "2022-04-21T14:52:04.634363Z",
     "shell.execute_reply": "2022-04-21T14:52:04.633541Z",
     "shell.execute_reply.started": "2022-04-21T14:52:02.007655Z"
    }
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the first 5 rows from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:04.636158Z",
     "iopub.status.busy": "2022-04-21T14:52:04.635894Z",
     "iopub.status.idle": "2022-04-21T14:52:04.650664Z",
     "shell.execute_reply": "2022-04-21T14:52:04.649922Z",
     "shell.execute_reply.started": "2022-04-21T14:52:04.636123Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop some column from the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:04.652284Z",
     "iopub.status.busy": "2022-04-21T14:52:04.652040Z",
     "iopub.status.idle": "2022-04-21T14:52:04.707574Z",
     "shell.execute_reply": "2022-04-21T14:52:04.706950Z",
     "shell.execute_reply.started": "2022-04-21T14:52:04.652251Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop([\"id\", \"date\", \"query\", \"user\"], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:04.709065Z",
     "iopub.status.busy": "2022-04-21T14:52:04.708823Z",
     "iopub.status.idle": "2022-04-21T14:52:04.716518Z",
     "shell.execute_reply": "2022-04-21T14:52:04.715820Z",
     "shell.execute_reply.started": "2022-04-21T14:52:04.709032Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " count the number of sentiments with respect to their tweet (4 stands for positive tweet and 0 stands for negative tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:04.718457Z",
     "iopub.status.busy": "2022-04-21T14:52:04.717942Z",
     "iopub.status.idle": "2022-04-21T14:52:04.737802Z",
     "shell.execute_reply": "2022-04-21T14:52:04.737197Z",
     "shell.execute_reply.started": "2022-04-21T14:52:04.718420Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add new column pre_clean_len to dataframe which is length of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:04.739260Z",
     "iopub.status.busy": "2022-04-21T14:52:04.739002Z",
     "iopub.status.idle": "2022-04-21T14:52:05.800107Z",
     "shell.execute_reply": "2022-04-21T14:52:05.799393Z",
     "shell.execute_reply.started": "2022-04-21T14:52:04.739226Z"
    }
   },
   "outputs": [],
   "source": [
    "df['pre_clean_len'] = [len(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding outliers using Box plot using pre_clean_len column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:05.801745Z",
     "iopub.status.busy": "2022-04-21T14:52:05.801505Z",
     "iopub.status.idle": "2022-04-21T14:52:06.061875Z",
     "shell.execute_reply": "2022-04-21T14:52:06.061228Z",
     "shell.execute_reply.started": "2022-04-21T14:52:05.801710Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.boxplot(df.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for any tweets greater than 140 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:06.063244Z",
     "iopub.status.busy": "2022-04-21T14:52:06.062888Z",
     "iopub.status.idle": "2022-04-21T14:52:06.106513Z",
     "shell.execute_reply": "2022-04-21T14:52:06.105836Z",
     "shell.execute_reply.started": "2022-04-21T14:52:06.063210Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.pre_clean_len > 140].head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, if you want you could remove these outlier tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Importing beautiful soup\n",
    "#remove @ mentions from tweets\n",
    "#remove URLs from tweets\n",
    "#converting words like isn't to is not\n",
    "#get only text from the tweets \n",
    "#remove utf-8-sig code\n",
    "#converting all into lower case\n",
    "#will replace non-alphabetic characters by space\n",
    "#Word Punct Tokenize and only consider words whose length is greater than 1\n",
    "#join the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:06.117753Z",
     "iopub.status.busy": "2022-04-21T14:52:06.116042Z",
     "iopub.status.idle": "2022-04-21T14:52:06.472998Z",
     "shell.execute_reply": "2022-04-21T14:52:06.472296Z",
     "shell.execute_reply.started": "2022-04-21T14:52:06.117714Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'        # remove @ mentions from tweets\n",
    "pat2 = r'https?://[^ ]+'        # remove URLs from tweets\n",
    "combined_pat = r'|'.join((pat1, pat2)) #addition of pat1 and pat2\n",
    "www_pat = r'www.[^ ]+'         # remove URLs from tweets\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",   # converting words like isn't to is not\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner(text):  # define tweet_cleaner function to clean the tweets\n",
    "    soup = BeautifulSoup(text, 'lxml')    # create beautiful soup object\n",
    "    souped = soup.get_text()   # get only text from the tweets \n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")    # remove utf-8-sig code\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed) # calling combined_pat\n",
    "    stripped = re.sub(www_pat, '', stripped) #remove URLs\n",
    "    lower_case = stripped.lower()      # converting all into lower case\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case) # converting words like isn't to is not\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)       # will replace # by space\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1] # Word Punct Tokenize and only consider words whose length is greater than 1\n",
    "    return (\" \".join(words)).strip() # join the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:52:06.479291Z",
     "iopub.status.busy": "2022-04-21T14:52:06.477049Z",
     "iopub.status.idle": "2022-04-21T14:59:00.733951Z",
     "shell.execute_reply": "2022-04-21T14:59:00.733258Z",
     "shell.execute_reply.started": "2022-04-21T14:52:06.479250Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Note that we have 1600000 instances. But processing so many instances will take a very very long time.\n",
    "#Hence, restricting to rather 50000 instances.\n",
    "limit=1600000\n",
    "import time; \n",
    "ms = time.time()\n",
    "#nums = [0,400000,800000,1200000,1600000] # used for batch processing tweets\n",
    "#nums = [0, 9999]\n",
    "clean_tweet_texts = [] # initialize list\n",
    "for i in range(0,limit): # batch process 1.6 million tweets \n",
    "    if i % 10000==0:\n",
    "        print(i, time.time()-ms)\n",
    "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))  # call tweet_cleaner function and pass parameter as all the tweets to clean the tweets and append cleaned tweets into clean_tweet_texts list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean_tweet_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:59:00.735523Z",
     "iopub.status.busy": "2022-04-21T14:59:00.735256Z",
     "iopub.status.idle": "2022-04-21T14:59:00.939885Z",
     "shell.execute_reply": "2022-04-21T14:59:00.938985Z",
     "shell.execute_reply.started": "2022-04-21T14:59:00.735476Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize word in clean_tweet_texts and append it to word_tokens list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T14:59:00.941516Z",
     "iopub.status.busy": "2022-04-21T14:59:00.941254Z",
     "iopub.status.idle": "2022-04-21T15:03:06.673084Z",
     "shell.execute_reply": "2022-04-21T15:03:06.672385Z",
     "shell.execute_reply.started": "2022-04-21T14:59:00.941469Z"
    }
   },
   "outputs": [],
   "source": [
    "word_tokens = [] # initialize list for tokens\n",
    "for word in clean_tweet_texts:  # for each word in clean_tweet_texts\n",
    "    word_tokens.append(word_tokenize(word)) #tokenize word in clean_tweet_texts and append it to word_tokens list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:03:06.674501Z",
     "iopub.status.busy": "2022-04-21T15:03:06.674245Z",
     "iopub.status.idle": "2022-04-21T15:03:06.725756Z",
     "shell.execute_reply": "2022-04-21T15:03:06.724971Z",
     "shell.execute_reply.started": "2022-04-21T15:03:06.674454Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:03:06.727367Z",
     "iopub.status.busy": "2022-04-21T15:03:06.727114Z",
     "iopub.status.idle": "2022-04-21T15:04:33.678748Z",
     "shell.execute_reply": "2022-04-21T15:04:33.677890Z",
     "shell.execute_reply.started": "2022-04-21T15:03:06.727334Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = [] # initialize list df1 to store words after lemmatization\n",
    "from nltk.stem import WordNetLemmatizer # import WordNetLemmatizer from nltk.stem\n",
    "lemmatizer = WordNetLemmatizer() # create an object of WordNetLemmatizer\n",
    "for l in word_tokens: # for loop for every tokens in word_token\n",
    "    b = [lemmatizer.lemmatize(q) for q in l] #for every tokens in word_token lemmatize word and giev it to b\n",
    "    df1.append(b) #append b to list df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:33.680856Z",
     "iopub.status.busy": "2022-04-21T15:04:33.679972Z",
     "iopub.status.idle": "2022-04-21T15:04:34.793178Z",
     "shell.execute_reply": "2022-04-21T15:04:34.792477Z",
     "shell.execute_reply.started": "2022-04-21T15:04:33.680812Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df1 =[] # initialize list clean_df1 to join word tokens after lemmatization\n",
    "for c in df1:  # for loop for each list in df1\n",
    "    a = \" \".join(c) # join words in list with space in between and give it to a\n",
    "    clean_df1.append(a) # append a to clean_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert clean_tweet_texts into dataframe and name it as clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:34.797706Z",
     "iopub.status.busy": "2022-04-21T15:04:34.797472Z",
     "iopub.status.idle": "2022-04-21T15:04:34.889610Z",
     "shell.execute_reply": "2022-04-21T15:04:34.888824Z",
     "shell.execute_reply.started": "2022-04-21T15:04:34.797679Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame(clean_df1,columns=['text']) # convert clean_tweet_texts into dataframe and name it as clean_df\n",
    "#clean_df['target'] = df.sentiment[:10000] # from earlier dataframe get the sentiments of each tweet and make a new column in clean_df as target and give it all the sentiment score\n",
    "#clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:34.891444Z",
     "iopub.status.busy": "2022-04-21T15:04:34.890930Z",
     "iopub.status.idle": "2022-04-21T15:04:35.735307Z",
     "shell.execute_reply": "2022-04-21T15:04:35.734395Z",
     "shell.execute_reply.started": "2022-04-21T15:04:34.891404Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df['clean_len'] = [len(t) for t in clean_df.text] # Again make a new coloumn in the dataframe and name it as clean_len which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:35.737047Z",
     "iopub.status.busy": "2022-04-21T15:04:35.736648Z",
     "iopub.status.idle": "2022-04-21T15:04:35.792282Z",
     "shell.execute_reply": "2022-04-21T15:04:35.791548Z",
     "shell.execute_reply.started": "2022-04-21T15:04:35.737008Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_df[clean_df.clean_len > 140].head(10) # again check if any tweet is more than 140 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:35.793828Z",
     "iopub.status.busy": "2022-04-21T15:04:35.793570Z",
     "iopub.status.idle": "2022-04-21T15:04:47.566111Z",
     "shell.execute_reply": "2022-04-21T15:04:47.565359Z",
     "shell.execute_reply.started": "2022-04-21T15:04:35.793794Z"
    }
   },
   "outputs": [],
   "source": [
    "target2 = [] # initialize list\n",
    "for i in range(0,limit): # batch process 1.6 million tweets \n",
    "    target2.append(df['sentiment'][i])\n",
    "clean_df['target']=target2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:47.567658Z",
     "iopub.status.busy": "2022-04-21T15:04:47.567314Z",
     "iopub.status.idle": "2022-04-21T15:04:48.252373Z",
     "shell.execute_reply": "2022-04-21T15:04:48.251391Z",
     "shell.execute_reply.started": "2022-04-21T15:04:47.567620Z"
    }
   },
   "outputs": [],
   "source": [
    "X = clean_df.text # get all the text in x variable\n",
    "y = clean_df.target # get all the sentiments into y variable\n",
    "print(X.shape) #print shape of x\n",
    "print(y.shape) # print shape of y\n",
    "from collections import Counter\n",
    "print(set(y)) # equals to list(set(words))\n",
    "print(Counter(y).values()) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform train and test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train is the tweets of training data, X_test is the testing tweets which we have to predict, y_train is the sentiments of tweets in the traing data and y_test is the sentiments of the tweets  which we will use to measure the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:48.254587Z",
     "iopub.status.busy": "2022-04-21T15:04:48.253941Z",
     "iopub.status.idle": "2022-04-21T15:04:48.671469Z",
     "shell.execute_reply": "2022-04-21T15:04:48.670704Z",
     "shell.execute_reply.started": "2022-04-21T15:04:48.254542Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split #from sklearn.cross_validation import train_test_split to split the data into training and tesing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state= 1) # split the data into traing and testing set where ratio is 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:48.673066Z",
     "iopub.status.busy": "2022-04-21T15:04:48.672834Z",
     "iopub.status.idle": "2022-04-21T15:04:48.896108Z",
     "shell.execute_reply": "2022-04-21T15:04:48.895410Z",
     "shell.execute_reply.started": "2022-04-21T15:04:48.673034Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = [x if x==0 else 1 for x in y_train.tolist()]\n",
    "y_test = [x if x==0 else 1 for x in y_test.tolist()]\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:48.897740Z",
     "iopub.status.busy": "2022-04-21T15:04:48.897477Z",
     "iopub.status.idle": "2022-04-21T15:04:50.328316Z",
     "shell.execute_reply": "2022-04-21T15:04:50.327610Z",
     "shell.execute_reply.started": "2022-04-21T15:04:48.897707Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train= np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Get Tf-idf object and save it as vect. We can select features from here we just have simply change \n",
    "#the ngram range to change the features also we can remove stop words over here with the help of stop parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:04:50.329984Z",
     "iopub.status.busy": "2022-04-21T15:04:50.329748Z",
     "iopub.status.idle": "2022-04-21T15:07:48.572603Z",
     "shell.execute_reply": "2022-04-21T15:07:48.571882Z",
     "shell.execute_reply.started": "2022-04-21T15:04:50.329951Z"
    }
   },
   "outputs": [],
   "source": [
    "#TF-IDF algoerithm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer  = TfidfVectorizer(analyzer = \"word\", ngram_range=(1,3))\n",
    "vectorizer.fit(X_train)\n",
    "X_train_tfidf = vectorizer.transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:07:48.574291Z",
     "iopub.status.busy": "2022-04-21T15:07:48.574042Z",
     "iopub.status.idle": "2022-04-21T15:07:50.247009Z",
     "shell.execute_reply": "2022-04-21T15:07:50.246325Z",
     "shell.execute_reply.started": "2022-04-21T15:07:48.574257Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # import Multinomial Naive Bayes model from sklearn.naive_bayes\n",
    "model_naive = MultinomialNB() \n",
    "model_naive.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:07:50.248617Z",
     "iopub.status.busy": "2022-04-21T15:07:50.248346Z",
     "iopub.status.idle": "2022-04-21T15:08:10.255838Z",
     "shell.execute_reply": "2022-04-21T15:08:10.255166Z",
     "shell.execute_reply.started": "2022-04-21T15:07:50.248581Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score  # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = model_naive, X = X_train_tfidf, y = y_train, cv = 10) # do K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() # measure the mean accuray of 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:08:10.257251Z",
     "iopub.status.busy": "2022-04-21T15:08:10.256882Z",
     "iopub.status.idle": "2022-04-21T15:08:10.628015Z",
     "shell.execute_reply": "2022-04-21T15:08:10.626383Z",
     "shell.execute_reply.started": "2022-04-21T15:08:10.257216Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_ = model_naive.predict(X_test_tfidf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:08:10.631332Z",
     "iopub.status.busy": "2022-04-21T15:08:10.630378Z",
     "iopub.status.idle": "2022-04-21T15:08:10.696055Z",
     "shell.execute_reply": "2022-04-21T15:08:10.695433Z",
     "shell.execute_reply.started": "2022-04-21T15:08:10.631262Z"
    }
   },
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "from sklearn import metrics # import metrics from sklearn\n",
    "metrics.accuracy_score(y_test, y_pred_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:08:10.701440Z",
     "iopub.status.busy": "2022-04-21T15:08:10.697243Z",
     "iopub.status.idle": "2022-04-21T15:08:10.804075Z",
     "shell.execute_reply": "2022-04-21T15:08:10.803454Z",
     "shell.execute_reply.started": "2022-04-21T15:08:10.701399Z"
    }
   },
   "outputs": [],
   "source": [
    "#Confucion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix # import confusion matrix from the sklearn.metrics\n",
    "confusion_matrix(y_test, y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:08:10.809821Z",
     "iopub.status.busy": "2022-04-21T15:08:10.807685Z",
     "iopub.status.idle": "2022-04-21T15:08:11.645734Z",
     "shell.execute_reply": "2022-04-21T15:08:11.644951Z",
     "shell.execute_reply.started": "2022-04-21T15:08:10.809782Z"
    }
   },
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:08:11.647419Z",
     "iopub.status.busy": "2022-04-21T15:08:11.647016Z",
     "iopub.status.idle": "2022-04-21T15:08:17.353210Z",
     "shell.execute_reply": "2022-04-21T15:08:17.352391Z",
     "shell.execute_reply.started": "2022-04-21T15:08:11.647366Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "pickle.dump(vectorizer, open(\"tfidf1.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:31:50.071220Z",
     "iopub.status.busy": "2022-04-21T15:31:50.070868Z",
     "iopub.status.idle": "2022-04-21T15:31:50.782160Z",
     "shell.execute_reply": "2022-04-21T15:31:50.781219Z",
     "shell.execute_reply.started": "2022-04-21T15:31:50.071176Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save Niave Base algorithm\n",
    "import pickle\n",
    "pickle.dump(model_naive, open(\"naive.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:08:17.866565Z",
     "iopub.status.busy": "2022-04-21T15:08:17.866319Z",
     "iopub.status.idle": "2022-04-21T15:08:17.873954Z",
     "shell.execute_reply": "2022-04-21T15:08:17.873212Z",
     "shell.execute_reply.started": "2022-04-21T15:08:17.866532Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # import Logistic Regression model from sklearn.linear_model\n",
    "logisticRegression = LogisticRegression(solver='lbfgs',max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:08:17.875248Z",
     "iopub.status.busy": "2022-04-21T15:08:17.875009Z",
     "iopub.status.idle": "2022-04-21T15:23:16.141547Z",
     "shell.execute_reply": "2022-04-21T15:23:16.140804Z",
     "shell.execute_reply.started": "2022-04-21T15:08:17.875211Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train the logistic regresion\n",
    "lg_model = logisticRegression.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:24:19.156873Z",
     "iopub.status.busy": "2022-04-21T15:24:19.156290Z",
     "iopub.status.idle": "2022-04-21T15:24:19.217787Z",
     "shell.execute_reply": "2022-04-21T15:24:19.217007Z",
     "shell.execute_reply.started": "2022-04-21T15:24:19.156835Z"
    }
   },
   "outputs": [],
   "source": [
    "#Testing prdc=iction\n",
    "y_pred_lg = lg_model.predict(X_test_tfidf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:24:22.525578Z",
     "iopub.status.busy": "2022-04-21T15:24:22.525196Z",
     "iopub.status.idle": "2022-04-21T15:24:22.588747Z",
     "shell.execute_reply": "2022-04-21T15:24:22.587918Z",
     "shell.execute_reply.started": "2022-04-21T15:24:22.525544Z"
    }
   },
   "outputs": [],
   "source": [
    "#confysion matrix\n",
    "confusion_matrix(y_pred_lg, y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:24:25.790233Z",
     "iopub.status.busy": "2022-04-21T15:24:25.789966Z",
     "iopub.status.idle": "2022-04-21T15:24:26.379833Z",
     "shell.execute_reply": "2022-04-21T15:24:26.378774Z",
     "shell.execute_reply.started": "2022-04-21T15:24:25.790202Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_pred_lg, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:31:32.317493Z",
     "iopub.status.busy": "2022-04-21T15:31:32.317215Z",
     "iopub.status.idle": "2022-04-21T15:31:32.448839Z",
     "shell.execute_reply": "2022-04-21T15:31:32.448096Z",
     "shell.execute_reply.started": "2022-04-21T15:31:32.317449Z"
    }
   },
   "outputs": [],
   "source": [
    "#save Logistic Regression model\n",
    "pickle.dump(lg_model, open(\"lg_model.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:28:51.016406Z",
     "iopub.status.busy": "2022-04-21T15:28:51.016118Z",
     "iopub.status.idle": "2022-04-21T15:28:51.020157Z",
     "shell.execute_reply": "2022-04-21T15:28:51.019375Z",
     "shell.execute_reply.started": "2022-04-21T15:28:51.016365Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC # import SVC model from sklearn.svm\n",
    "svm = LinearSVC(random_state=0) # get object of SVC model with random_state parameter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:28:51.945761Z",
     "iopub.status.busy": "2022-04-21T15:28:51.944997Z",
     "iopub.status.idle": "2022-04-21T15:29:34.439101Z",
     "shell.execute_reply": "2022-04-21T15:29:34.438328Z",
     "shell.execute_reply.started": "2022-04-21T15:28:51.945723Z"
    }
   },
   "outputs": [],
   "source": [
    "#SVM train\n",
    "svm_model = svm.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:30:27.827055Z",
     "iopub.status.busy": "2022-04-21T15:30:27.826584Z",
     "iopub.status.idle": "2022-04-21T15:30:27.915894Z",
     "shell.execute_reply": "2022-04-21T15:30:27.915019Z",
     "shell.execute_reply.started": "2022-04-21T15:30:27.827012Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prediction\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:30:48.546357Z",
     "iopub.status.busy": "2022-04-21T15:30:48.546077Z",
     "iopub.status.idle": "2022-04-21T15:30:48.611845Z",
     "shell.execute_reply": "2022-04-21T15:30:48.611207Z",
     "shell.execute_reply.started": "2022-04-21T15:30:48.546330Z"
    }
   },
   "outputs": [],
   "source": [
    "#confysion matrix\n",
    "confusion_matrix(y_pred_svm, y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:31:20.961187Z",
     "iopub.status.busy": "2022-04-21T15:31:20.960919Z",
     "iopub.status.idle": "2022-04-21T15:31:21.537165Z",
     "shell.execute_reply": "2022-04-21T15:31:21.536379Z",
     "shell.execute_reply.started": "2022-04-21T15:31:20.961159Z"
    }
   },
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "print(classification_report(y_pred_svm, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T15:32:19.986167Z",
     "iopub.status.busy": "2022-04-21T15:32:19.985904Z",
     "iopub.status.idle": "2022-04-21T15:32:20.110328Z",
     "shell.execute_reply": "2022-04-21T15:32:20.109535Z",
     "shell.execute_reply.started": "2022-04-21T15:32:19.986139Z"
    }
   },
   "outputs": [],
   "source": [
    "#save Logistic Regression model\n",
    "pickle.dump(svm_model, open(\"svm_model.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
